# ML From Scratch

Ce projet contient des implÃ©mentations d'algorithmes de Machine Learning et de Deep Learning codÃ©s "from scratch" en Python.

## Objectif
MaÃ®triser en profondeur le fonctionnement interne des algorithmes et dÃ©velopper une solide comprÃ©hension des fondamentaux.

## Structure du Projet
- `src/mlfs/` : Le package Python principal contenant tout le code source.
  - `utils/` : Fonctions utilitaires partagÃ©es (mÃ©triques, manipulation de donnÃ©es).
  - `supervised/` : Algorithmes d'apprentissage supervisÃ© (rÃ©gression, classification).
  - `unsupervised/` : Algorithmes d'apprentissage non-supervisÃ© (clustering, rÃ©duction de dimension).
  - `deep_learning/` : Bases pour construire des rÃ©seaux de neurones.
- `examples/` : Scripts exÃ©cutables montrant comment utiliser les implÃ©mentations.
- `tests/` : Tests unitaires pour valider le fonctionnement du code.
- `notebooks/` : Notebooks pour l'exploration, la visualisation et l'expÃ©rimentation.
- `data/` : (Optionnel) Pour stocker des jeux de donnÃ©es.

## Pour Commencer
1.  CrÃ©er et activer un environnement virtuel :
    ```bash
    python -m venv venv
    source venv/bin/activate  # Sur Windows: venv\Scripts\activate
    ```
2.  Installer les dÃ©pendances :
    ```bash
    pip install -r requirements.txt
    ```
3.  Lancer un exemple :
    ```bash
    python examples/run_linear_regression.py
    ```

## Feuille de Route des Algorithmes

Voici la liste des algorithmes Ã  implÃ©menter, classÃ©s par catÃ©gorie et par ordre de difficultÃ© croissant. Il est recommandÃ© de suivre cet ordre pour construire ses connaissances de maniÃ¨re progressive.

### LÃ©gende de DifficultÃ©
- ğŸŸ¢ **Fondamental** : Les bases du ML, incontournables.
- ğŸŸ¡ **Essentiel** : Algorithmes courants et trÃ¨s utiles.
- ğŸŸ  **AvancÃ©** : Concepts plus complexes, souvent plus performants.
- ğŸ”´ **Expert** : ImplÃ©mentations difficiles, excellentes pour briller en entretien.

---

### ğŸ“ˆ 01 - Apprentissage SupervisÃ© : RÃ©gression
*PrÃ©dire une valeur continue.*

- [X] ğŸŸ¢ **RÃ©gression LinÃ©aire** : Le "Hello World" du ML. Descente de gradient.
- [ ] ğŸŸ¡ **RÃ©gression Polynomiale** : GÃ©rer la non-linÃ©aritÃ© en ajoutant des features.
- [ ] ğŸŸ¡ **Ridge & Lasso Regression** : Comprendre la rÃ©gularisation pour Ã©viter l'overfitting.

---

### ğŸ·ï¸ 02 - Apprentissage SupervisÃ© : Classification
*PrÃ©dire une catÃ©gorie discrÃ¨te.*

- [ ] ğŸŸ¢ **RÃ©gression Logistique** : La base de la classification binaire.
- [ ] ğŸŸ¢ **K-Plus Proches Voisins (KNN)** : Algorithme simple basÃ© sur la distance.
- [ ] ğŸŸ¡ **NaÃ¯ve Bayes** : Classifieur probabiliste rapide et efficace.
- [ ] ğŸŸ¡ **Arbre de DÃ©cision** : Le bloc de construction des modÃ¨les ensemblistes.
- [ ] ğŸŸ  **Support Vector Machines (SVM)** : Classifieur puissant basÃ© sur la notion de marge maximale.

---

### ğŸ” 03 - Apprentissage Non-SupervisÃ©
*Explorer la structure des donnÃ©es sans Ã©tiquettes.*

- [ ] ğŸŸ¡ **K-Moyennes (K-Means)** : L'algorithme de clustering le plus cÃ©lÃ¨bre.
- [ ] ğŸŸ  **Analyse en Composantes Principales (PCA)** : La mÃ©thode de rÃ©fÃ©rence pour la rÃ©duction de dimension.
- [ ] ğŸŸ  **Gaussian Mixture Models (GMM)** : ModÃ¨le de clustering probabiliste plus flexible que K-Means.

---

### ğŸ§  04 - Deep Learning Fondamentaux
*Construire des rÃ©seaux de neurones from scratch.*

- [ ] ğŸŸ¡ **Le Perceptron** : Le neurone artificiel de base.
- [ ] ğŸŸ¡ **Fonctions d'Activation** : ImplÃ©menter Sigmoid, ReLU, Tanh.
- [ ] ğŸŸ¡ **Fonctions de Perte** : ImplÃ©menter MSE, Cross-Entropy.
- [ ] ğŸŸ  **Multi-Layer Perceptron (MLP)** : Assembler le tout pour crÃ©er un rÃ©seau de neurones simple (rÃ©seau "dense").
- [ ] ğŸŸ  **Backpropagation** : L'algorithme d'optimisation au cÅ“ur de l'entraÃ®nement des rÃ©seaux de neurones.
- [ ] ğŸ”´ **Gradient Boosting (Machine)** : Un des algorithmes les plus puissants pour les donnÃ©es tabulaires.

---